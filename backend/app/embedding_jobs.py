"""
Jobs batch pour la g√©n√©ration d'embeddings
Permet de lancer des t√¢ches de traitement d'embeddings en arri√®re-plan
"""

import asyncio
import os
from typing import Optional
from sqlalchemy.orm import Session
from .database import get_db
from .embeddings import process_batch_embeddings, get_embedding_stats, DEFAULT_EMBEDDING_MODEL
from .models import DocumentChunk

class EmbeddingJobManager:
    """Gestionnaire des jobs d'embedding"""
    
    def __init__(self):
        self.current_job = None
        self.is_running = False
        self.job_stats = {}
    
    async def run_embedding_job(
        self,
        db: Session,
        model: str = DEFAULT_EMBEDDING_MODEL,
        batch_size: int = 5,  # Taille r√©duite pour √©viter le rate limiting
        max_chunks: Optional[int] = None,
        force_reprocess: bool = False
    ) -> dict:
        """
        Lance un job de g√©n√©ration d'embeddings
        
        Args:
            db: Session de base de donn√©es
            model: Mod√®le OpenAI √† utiliser
            batch_size: Nombre de chunks √† traiter par batch
            max_chunks: Limite maximale de chunks √† traiter
            force_reprocess: Force le retraitement des chunks existants
            
        Returns:
            Statistiques du job
        """
        if self.is_running:
            return {
                "error": "Un job d'embedding est d√©j√† en cours",
                "current_job": self.current_job
            }
        
        self.is_running = True
        self.current_job = {
            "started_at": asyncio.get_event_loop().time(),
            "model": model,
            "batch_size": batch_size,
            "max_chunks": max_chunks,
            "force_reprocess": force_reprocess
        }
        
        try:
            print(f"üöÄ D√©marrage du job d'embedding (mod√®le: {model})")
            
            # Si force_reprocess, supprimer les embeddings existants
            if force_reprocess:
                print("üîÑ Force reprocess: suppression des embeddings existants...")
                chunks_to_reset = db.query(DocumentChunk).filter(
                    DocumentChunk.embedding.isnot(None)
                ).all()
                
                for chunk in chunks_to_reset:
                    chunk.embedding = None
                    chunk.embedding_model = None
                    chunk.embedding_created_at = None
                
                db.commit()
                print(f"‚úÖ {len(chunks_to_reset)} embeddings supprim√©s")
            
            # Lancer le traitement batch
            stats = await process_batch_embeddings(
                db=db,
                model=model,
                batch_size=batch_size,
                max_chunks=max_chunks
            )
            
            # Mettre √† jour les statistiques du job
            self.current_job.update({
                "completed_at": asyncio.get_event_loop().time(),
                "status": "completed",
                "stats": stats
            })
            
            # Calculer la dur√©e
            duration = self.current_job["completed_at"] - self.current_job["started_at"]
            self.current_job["duration_seconds"] = round(duration, 2)
            
            print(f"‚úÖ Job d'embedding termin√© en {duration:.2f}s")
            
            return self.current_job
            
        except Exception as e:
            error_msg = f"Erreur lors du job d'embedding: {e}"
            print(f"‚ùå {error_msg}")
            
            self.current_job.update({
                "completed_at": asyncio.get_event_loop().time(),
                "status": "failed",
                "error": str(e)
            })
            
            return self.current_job
            
        finally:
            self.is_running = False
            self.job_stats = self.current_job.copy()
    
    def get_job_status(self) -> dict:
        """Retourne le statut du job actuel"""
        if self.is_running:
            current_time = asyncio.get_event_loop().time()
            duration = current_time - self.current_job["started_at"]
            
            return {
                "status": "running",
                "duration_seconds": round(duration, 2),
                "job_details": self.current_job
            }
        elif self.job_stats:
            return {
                "status": "completed",
                "last_job": self.job_stats
            }
        else:
            return {
                "status": "idle",
                "message": "Aucun job d'embedding ex√©cut√©"
            }

# Instance globale du gestionnaire
embedding_job_manager = EmbeddingJobManager()

async def schedule_embedding_job(
    db: Session,
    model: str = DEFAULT_EMBEDDING_MODEL,
    batch_size: int = 5,
    max_chunks: Optional[int] = None,
    force_reprocess: bool = False
) -> dict:
    """
    Planifie un job d'embedding
    """
    return await embedding_job_manager.run_embedding_job(
        db=db,
        model=model,
        batch_size=batch_size,
        max_chunks=max_chunks,
        force_reprocess=force_reprocess
    )

def get_embedding_job_status() -> dict:
    """
    Retourne le statut des jobs d'embedding
    """
    return embedding_job_manager.get_job_status()

async def auto_generate_embeddings_for_new_chunks(db: Session, chunk_ids: list) -> dict:
    """
    G√©n√®re automatiquement les embeddings pour de nouveaux chunks
    """
    if not chunk_ids:
        return {"message": "Aucun chunk √† traiter"}
    
    # R√©cup√©rer les chunks sp√©cifiques
    chunks = db.query(DocumentChunk).filter(
        DocumentChunk.id.in_(chunk_ids),
        DocumentChunk.embedding.is_(None)
    ).all()
    
    if not chunks:
        return {"message": "Aucun chunk sans embedding trouv√©"}
    
    from .embeddings import process_chunk_embedding
    
    stats = {
        "total_chunks": len(chunks),
        "processed": 0,
        "errors": 0
    }
    
    print(f"üîÑ G√©n√©ration automatique d'embeddings pour {len(chunks)} nouveaux chunks")
    
    for chunk in chunks:
        try:
            success = await process_chunk_embedding(chunk, db)
            if success:
                stats["processed"] += 1
            else:
                stats["errors"] += 1
                
            # Petite pause pour √©viter le rate limiting
            await asyncio.sleep(0.2)
            
        except Exception as e:
            print(f"‚ùå Erreur chunk {chunk.id}: {e}")
            stats["errors"] += 1
    
    return stats

def check_embedding_requirements() -> dict:
    """
    V√©rifie les pr√©requis pour les embeddings
    """
    requirements = {
        "openai_api_key": bool(os.getenv("OPENAI_API_KEY")),
        "openai_package": False,
        "numpy_package": False
    }
    
    try:
        import openai
        requirements["openai_package"] = True
    except ImportError:
        pass
    
    try:
        import numpy
        requirements["numpy_package"] = True
    except ImportError:
        pass
    
    requirements["all_requirements_met"] = all(requirements.values())
    
    return requirements 