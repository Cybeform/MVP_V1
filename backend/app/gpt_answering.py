"""
Module de g√©n√©ration de r√©ponses intelligentes avec GPT-4o
Analyse les passages trouv√©s et g√©n√®re des r√©ponses avec citations
Sp√©cialis√© pour les documents CCTP et le domaine du BTP
"""

import os
import time
import json
import re
from typing import List, Dict, Any, Optional, Tuple
from .schemas import QAChunkResult, QACitation

def get_openai_client():
    """Initialise et retourne le client OpenAI"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY non configur√©e")
    
    try:
        import openai
        return openai.OpenAI(api_key=api_key)
    except ImportError:
        raise ImportError("Package 'openai' non install√©")

def create_specialized_qa_prompt(question: str, chunks: List[QAChunkResult], document_name: str) -> str:
    """
    Cr√©e un prompt sp√©cialis√© pour l'analyse de documents CCTP avec expertise BTP
    """
    
    # Contexte m√©tier sp√©cialis√©
    context_intro = f"""Vous √™tes un expert en analyse de documents techniques du BTP, sp√©cialis√© dans les CCTP (Cahier des Clauses Techniques Particuli√®res).

CONTEXTE M√âTIER :
- Les CCTP d√©finissent les sp√©cifications techniques des travaux
- Ils contiennent des prescriptions sur les mat√©riaux, m√©thodes, performances et contr√¥les
- Ils sont organis√©s par lots de travaux (gros ≈ìuvre, menuiserie, plomberie, etc.)
- Chaque prescription doit respecter les normes en vigueur (DTU, NF, Eurocodes)

DOCUMENT ANALYS√â : {document_name}
QUESTION POS√âE : {question}

PASSAGES PERTINENTS EXTRAITS DU DOCUMENT :

"""
    
    # Analyser les chunks pour identifier les th√®mes
    themes = analyze_content_themes(chunks)
    
    # Formater les passages avec enrichissement contextuel
    passages = []
    for i, chunk in enumerate(chunks, 1):
        passage = f"üîç PASSAGE {i}:\n"
        
        # M√©tadonn√©es enrichies
        metadata = []
        if chunk.lot:
            metadata.append(f"üìÅ Lot: {chunk.lot}")
        if chunk.article:
            metadata.append(f"üìã Article: {chunk.article}")
        if chunk.page_number:
            metadata.append(f"üìÑ Page: {chunk.page_number}")
        
        if metadata:
            passage += f"[{' | '.join(metadata)}]\n"
        
        # Score de pertinence avec interpr√©tation
        score_interpretation = get_score_interpretation(chunk.similarity_score)
        passage += f"üìä Pertinence: {chunk.similarity_score:.2%} ({score_interpretation})\n\n"
        
        passage += f"üìù CONTENU:\n{chunk.text}\n\n"
        passage += "‚îÄ" * 80 + "\n\n"
        
        passages.append(passage)
    
    # Instructions sp√©cialis√©es et contextuelles
    instructions = f"""
üéØ INSTRUCTIONS POUR LA R√âPONSE :

1. **ANALYSE TECHNIQUE** :
   - Analysez les passages en tant qu'expert BTP/CCTP
   - Identifiez les exigences techniques, normes et prescriptions
   - Relevez les performances, tol√©rances et m√©thodes de contr√¥le
   - Notez les mat√©riaux sp√©cifi√©s et leurs caract√©ristiques

2. **STRUCTURE DE R√âPONSE** :
   - Commencez par une r√©ponse directe et synth√©tique
   - D√©veloppez avec les d√©tails techniques pertinents
   - Citez pr√©cis√©ment vos sources (ex: "Selon le PASSAGE 2, lot menuiserie...")
   - Mentionnez les normes, DTU ou r√©f√©rences techniques quand pertinentes

3. **DOMAINES D'EXPERTISE √Ä MOBILISER** :
   - Mat√©riaux de construction et leurs propri√©t√©s
   - Techniques de mise en ≈ìuvre et contr√¥les qualit√©
   - Normes et r√©glementations du BTP (DTU, NF, RT, etc.)
   - Performances thermiques, acoustiques, m√©caniques
   - Proc√©dures d'essais et de r√©ception

4. **TH√àMES IDENTIFI√âS DANS LE DOCUMENT** :
{format_themes_for_prompt(themes)}

5. **CITATIONS ET TRA√áABILIT√â** :
   - Citez syst√©matiquement vos sources avec le num√©ro de passage
   - Indiquez le lot et la page quand disponibles
   - Diff√©renciez les exigences obligatoires des recommandations
   - Mentionnez les r√©f√©rences normatives cit√©es

6. **QUALIT√â DE R√âPONSE** :
   - Si l'information est incompl√®te, indiquez-le clairement
   - Distinguez les faits des interpr√©tations
   - Signaler les contradictions ou ambigu√Øt√©s √©ventuelles
   - Proposez des pr√©cisions compl√©mentaires si n√©cessaire

7. **ADAPTATION AU CONTEXTE** :
   - Utilisez le vocabulaire technique appropri√©
   - Respectez la hi√©rarchie des exigences (obligatoire/recommand√©)
   - Tenez compte du niveau de pr√©cision de la question

üöÄ R√âPONDEZ MAINTENANT √† la question en appliquant votre expertise BTP et en suivant ces instructions.
"""
    
    return context_intro + "".join(passages) + instructions

def analyze_content_themes(chunks: List[QAChunkResult]) -> Dict[str, int]:
    """
    Analyse les th√®mes principaux dans les chunks pour enrichir le contexte
    """
    themes = {
        "mat√©riaux": 0,
        "performances": 0,
        "contr√¥les": 0,
        "normes": 0,
        "mise_en_oeuvre": 0,
        "essais": 0,
        "r√©ception": 0,
        "s√©curit√©": 0
    }
    
    keywords = {
        "mat√©riaux": ["mat√©riau", "mat√©riaux", "b√©ton", "acier", "bois", "aluminium", "PVC"],
        "performances": ["performance", "r√©sistance", "√©tanch√©it√©", "isolation", "thermique", "acoustique"],
        "contr√¥les": ["contr√¥le", "v√©rification", "surveillance", "inspection", "validation"],
        "normes": ["DTU", "NF", "norme", "r√©glementation", "RT", "eurocode"],
        "mise_en_oeuvre": ["pose", "installation", "mise en ≈ìuvre", "montage", "assemblage"],
        "essais": ["essai", "test", "mesure", "caract√©risation"],
        "r√©ception": ["r√©ception", "livraison", "conformit√©", "acceptation"],
        "s√©curit√©": ["s√©curit√©", "protection", "EPI", "pr√©vention"]
    }
    
    for chunk in chunks:
        text_lower = chunk.text.lower()
        for theme, words in keywords.items():
            for word in words:
                if word in text_lower:
                    themes[theme] += 1
    
    return themes

def get_score_interpretation(score: float) -> str:
    """
    Interpr√®te le score de similarit√© pour le contexte
    """
    if score >= 0.8:
        return "tr√®s pertinent"
    elif score >= 0.6:
        return "pertinent"
    elif score >= 0.4:
        return "moyennement pertinent"
    else:
        return "faiblement pertinent"

def format_themes_for_prompt(themes: Dict[str, int]) -> str:
    """
    Formate les th√®mes identifi√©s pour le prompt
    """
    relevant_themes = [theme for theme, count in themes.items() if count > 0]
    if relevant_themes:
        return f"   - Th√®mes d√©tect√©s: {', '.join(relevant_themes)}"
    else:
        return "   - Aucun th√®me sp√©cifique identifi√©"

async def generate_gpt_answer(
    question: str, 
    chunks: List[QAChunkResult], 
    document_name: str,
    model: str = "gpt-4o"
) -> Tuple[str, List[QACitation], str, int]:
    """
    G√©n√®re une r√©ponse sp√©cialis√©e avec GPT-4o pour le domaine BTP/CCTP
    
    Returns:
        (answer, citations, confidence, processing_time_ms)
    """
    start_time = time.time()
    
    if not chunks:
        return (
            "Aucune information pertinente trouv√©e dans le document pour r√©pondre √† cette question. "
            "Veuillez reformuler votre question ou v√©rifier si le sujet est trait√© dans ce document CCTP.",
            [],
            "faible",
            int((time.time() - start_time) * 1000)
        )
    
    try:
        client = get_openai_client()
        
        # Cr√©er le prompt sp√©cialis√©
        prompt = create_specialized_qa_prompt(question, chunks, document_name)
        
        # Configuration optimis√©e pour les r√©ponses techniques
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "Vous √™tes un expert en BTP et documents techniques CCTP. Vos r√©ponses sont pr√©cises, techniques et toujours sourc√©es. Vous ma√Ætrisez les normes, DTU, mat√©riaux et techniques de construction."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.2,  # Tr√®s faible pour plus de pr√©cision
            max_tokens=2000,  # Augment√© pour des r√©ponses plus d√©taill√©es
            top_p=0.9,
            frequency_penalty=0.1,  # √âviter les r√©p√©titions
            presence_penalty=0.1    # Encourager la diversit√©
        )
        
        answer = response.choices[0].message.content
        processing_time = int((time.time() - start_time) * 1000)
        
        # Extraire les citations de la r√©ponse avec am√©lioration
        citations = extract_enhanced_citations(answer, chunks)
        
        # Calculer la confiance am√©lior√©e
        confidence = calculate_enhanced_confidence(chunks, citations, answer)
        
        return answer, citations, confidence, processing_time
        
    except Exception as e:
        error_msg = f"Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        # R√©ponse de fallback am√©lior√©e
        fallback_answer = generate_fallback_answer(chunks, question)
        
        return (
            fallback_answer,
            [],
            "faible",
            int((time.time() - start_time) * 1000)
        )

def generate_fallback_answer(chunks: List[QAChunkResult], question: str) -> str:
    """
    G√©n√®re une r√©ponse de fallback structur√©e en cas d'erreur GPT
    """
    fallback = f"**R√©ponse automatique g√©n√©r√©e**\n\n"
    fallback += f"En r√©ponse √† votre question : '{question}'\n\n"
    fallback += f"Voici les informations les plus pertinentes trouv√©es dans le document :\n\n"
    
    for i, chunk in enumerate(chunks[:5], 1):  # Top 5 chunks
        fallback += f"**üìã Information {i}** "
        
        # Informations contextuelles
        context_info = []
        if chunk.lot:
            context_info.append(f"Lot {chunk.lot}")
        if chunk.page_number:
            context_info.append(f"Page {chunk.page_number}")
        if chunk.similarity_score:
            context_info.append(f"Pertinence {chunk.similarity_score:.0%}")
            
        if context_info:
            fallback += f"({', '.join(context_info)}):\n"
        else:
            fallback += ":\n"
        
        # Texte tronqu√© intelligemment
        text = chunk.text
        if len(text) > 300:
            # Trouver une phrase compl√®te proche de 300 caract√®res
            sentences = re.split(r'[.!?]\s+', text)
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence) > 300:
                    break
                truncated += sentence + ". "
            text = truncated.strip() + "..."
        
        fallback += f"*{text}*\n\n"
    
    fallback += "---\n"
    fallback += "üí° **Conseil**: Pour une analyse plus pr√©cise, reformulez votre question ou contactez un expert BTP."
    
    return fallback

def extract_enhanced_citations(answer: str, chunks: List[QAChunkResult]) -> List[QACitation]:
    """
    Extrait les citations de la r√©ponse GPT avec am√©lioration de la d√©tection
    """
    citations = []
    
    # Patterns de citation am√©lior√©s
    patterns = [
        r"PASSAGE\s+(\d+)",  # PASSAGE 1, PASSAGE 2, etc.
        r"passage\s+(\d+)",  # passage 1, passage 2, etc.
        r"selon.*?passage\s+(\d+)",  # selon le passage 1
        r"d'apr√®s.*?passage\s+(\d+)",  # d'apr√®s le passage 1
        r"passage\s+n¬∞?\s*(\d+)",  # passage n¬∞1, passage n¬∞ 1
        r"extrait\s+(\d+)",  # extrait 1
        r"section\s+(\d+)",  # section 1
        r"information\s+(\d+)"  # information 1
    ]
    
    # Rechercher tous les patterns
    referenced_chunks = set()
    for pattern in patterns:
        matches = re.finditer(pattern, answer, re.IGNORECASE)
        for match in matches:
            try:
                chunk_number = int(match.group(1))
                if 1 <= chunk_number <= len(chunks):
                    referenced_chunks.add(chunk_number - 1)  # Index 0-based
            except (ValueError, IndexError):
                continue
    
    # Cr√©er les citations pour les chunks r√©f√©renc√©s
    for chunk_index in sorted(referenced_chunks):
        chunk = chunks[chunk_index]
        
        # Extraire un extrait repr√©sentatif
        excerpt = extract_relevant_excerpt(chunk.text, answer)
        
        citation = QACitation(
            chunk_id=chunk.chunk_id,
            lot=chunk.lot,
            page=chunk.page_number,
            excerpt=excerpt
        )
        citations.append(citation)
    
    # Si aucune citation explicite trouv√©e, utiliser les meilleurs chunks
    if not citations and chunks:
        # Prendre les 3 chunks avec les meilleurs scores
        top_chunks = sorted(chunks, key=lambda x: x.similarity_score, reverse=True)[:3]
        for chunk in top_chunks:
            excerpt = extract_relevant_excerpt(chunk.text, answer)
            citation = QACitation(
                chunk_id=chunk.chunk_id,
                lot=chunk.lot,
                page=chunk.page_number,
                excerpt=excerpt
            )
            citations.append(citation)
    
    return citations

def extract_relevant_excerpt(chunk_text: str, answer: str) -> str:
    """
    Extrait l'extrait le plus pertinent d'un chunk par rapport √† la r√©ponse
    """
    # Limiter la longueur de l'extrait
    max_excerpt_length = 200
    
    if len(chunk_text) <= max_excerpt_length:
        return chunk_text
    
    # Essayer de trouver des phrases compl√®tes qui correspondent √† la r√©ponse
    sentences = re.split(r'[.!?]\s+', chunk_text)
    
    # Chercher la phrase la plus pertinente bas√©e sur les mots cl√©s de la r√©ponse
    answer_words = set(answer.lower().split())
    best_sentence = ""
    best_score = 0
    
    for sentence in sentences:
        if len(sentence.strip()) < 20:  # Ignorer les phrases trop courtes
            continue
            
        sentence_words = set(sentence.lower().split())
        overlap = len(answer_words.intersection(sentence_words))
        
        if overlap > best_score:
            best_score = overlap
            best_sentence = sentence.strip()
    
    # Si on a trouv√© une bonne phrase
    if best_sentence and len(best_sentence) <= max_excerpt_length:
        return best_sentence
    
    # Sinon, prendre le d√©but du chunk
    excerpt = chunk_text[:max_excerpt_length]
    # Trouver la derni√®re phrase compl√®te
    last_sentence_end = max(
        excerpt.rfind('.'),
        excerpt.rfind('!'),
        excerpt.rfind('?')
    )
    
    if last_sentence_end > max_excerpt_length * 0.7:  # Au moins 70% du texte
        excerpt = excerpt[:last_sentence_end + 1]
    else:
        excerpt += "..."
    
    return excerpt

def calculate_enhanced_confidence(chunks: List[QAChunkResult], citations: List[QACitation], answer: str) -> str:
    """
    Calcule le niveau de confiance am√©lior√© bas√© sur plusieurs facteurs
    """
    if not chunks:
        return "faible"
    
    # Facteur 1: Scores de similarit√© moyens
    avg_score = sum(chunk.similarity_score for chunk in chunks) / len(chunks)
    
    # Facteur 2: Nombre de chunks utilis√©s
    chunks_factor = min(len(chunks) / 5, 1.0)  # Normalis√© sur 5 chunks
    
    # Facteur 3: Pr√©sence de citations explicites
    citation_factor = min(len(citations) / 3, 1.0) if citations else 0
    
    # Facteur 4: Longueur et d√©tail de la r√©ponse
    answer_length_factor = min(len(answer) / 1000, 1.0) if answer else 0
    
    # Facteur 5: Diversit√© des sources (diff√©rents lots/pages)
    unique_sources = set()
    for chunk in chunks:
        source_id = f"{chunk.lot or 'unknown'}_{chunk.page_number or 0}"
        unique_sources.add(source_id)
    diversity_factor = min(len(unique_sources) / 3, 1.0)
    
    # Facteur 6: Pr√©sence de termes techniques dans l'answer
    technical_terms = [
        "norme", "DTU", "performance", "contr√¥le", "essai", 
        "mat√©riau", "prescription", "tol√©rance", "r√©sistance"
    ]
    technical_count = sum(1 for term in technical_terms if term.lower() in answer.lower())
    technical_factor = min(technical_count / 3, 1.0)
    
    # Calcul de confiance pond√©r√©
    confidence_score = (
        avg_score * 0.3 +                # 30% pour la similarit√©
        chunks_factor * 0.2 +            # 20% pour le nombre de chunks
        citation_factor * 0.2 +          # 20% pour les citations
        answer_length_factor * 0.1 +     # 10% pour la longueur
        diversity_factor * 0.1 +         # 10% pour la diversit√©
        technical_factor * 0.1           # 10% pour les termes techniques
    )
    
    # Classification finale
    if confidence_score >= 0.75:
        return "haute"
    elif confidence_score >= 0.5:
        return "moyenne"
    else:
        return "faible"

def calculate_confidence(chunks: List[QAChunkResult], citations: List[QACitation]) -> str:
    """
    Calcule le niveau de confiance bas√© sur les scores de similarit√© (version originale)
    """
    if not chunks:
        return "faible"
    
    # Score moyen des chunks utilis√©s
    if citations:
        # Utiliser les chunks r√©f√©renc√©s
        referenced_chunk_ids = {c.chunk_id for c in citations}
        relevant_chunks = [c for c in chunks if c.chunk_id in referenced_chunk_ids]
    else:
        # Utiliser les 3 premiers chunks
        relevant_chunks = chunks[:3]
    
    if not relevant_chunks:
        return "faible"
    
    avg_score = sum(chunk.similarity_score for chunk in relevant_chunks) / len(relevant_chunks)
    
    if avg_score >= 0.8:
        return "haute"
    elif avg_score >= 0.6:
        return "moyenne"
    else:
        return "faible"

def format_citations_for_display(citations: List[QACitation]) -> str:
    """
    Formate les citations pour l'affichage
    """
    if not citations:
        return "Aucune citation disponible"
    
    formatted = []
    for i, citation in enumerate(citations, 1):
        citation_text = f"**Citation {i}**"
        
        metadata = []
        if citation.lot:
            metadata.append(f"Lot: {citation.lot}")
        if citation.page:
            metadata.append(f"Page: {citation.page}")
        
        if metadata:
            citation_text += f" ({', '.join(metadata)})"
        
        citation_text += f"\n{citation.excerpt}\n"
        formatted.append(citation_text)
    
    return "\n".join(formatted)

def validate_gpt_response(answer: str, question: str) -> bool:
    """
    Valide que la r√©ponse GPT-4o est pertinente
    """
    if not answer or len(answer.strip()) < 10:
        return False
    
    # V√©rifier que la r√©ponse ne contient pas trop d'erreurs standard
    error_phrases = [
        "je ne peux pas r√©pondre",
        "information non disponible",
        "impossible de r√©pondre",
        "donn√©es insuffisantes"
    ]
    
    answer_lower = answer.lower()
    error_count = sum(1 for phrase in error_phrases if phrase in answer_lower)
    
    return error_count < 2  # Tol√©rer quelques phrases d'erreur 